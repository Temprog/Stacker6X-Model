# -*- coding: utf-8 -*-
"""data_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bRqb9jLAvpPNs2qzbpk_lOHUZQFyVDN7

# Data Preprocessing
"""

# imports from utils.imports
from utils.imports_py import pandas, word_tokenize, stopwords, re, resample

df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Detection_SQLI_XSS.csv')

# A copy of main dataframe is made for future uses
df_main=df.copy()

# Changing the column from was auto-changed to float values like 1.0, 0.0 values back to integers to ensure consistency
df1['SQLInjection'] = df1['SQLInjection'].astype(int)
df1['XSS'] = df1['XSS'].astype(int)
df1['Normal'] = df1['Normal'].astype(int)

df1.shape

#Since the dataFram was randomised, the indexes got randomised as well.
#The machine learning might fail if this stays like this. To correct this,the indexes of the new dataframe are reset
df1 = df1.reset_index(drop=True)
df1.head(5)

# Conversion of all payloads to lowercase for case consistency.
df1['Payload'] = df1['Payload'].str.lower()

# Removes unnecessary strip leading and trailing whitespaces and collapse multiple spaces.
df1['Payload'] = df1['Payload'].str.strip().replace(r'\s+', ' ', regex=True)

# Removes non-ASCII or invisible characters that might not contribute meaningfully to detection.
df1['Payload'] = df1['Payload'].replace(r'[^\x20-\x7E]', '', regex=True)

# Decode URL-encoded strings (%20 -> space, %27 -> '), as malicious payloads are often encoded to bypass filters.
import urllib.parse
df1['Payload'] = df1['Payload'].apply(urllib.parse.unquote)

# Removing duplicate rows in Payloads column specifically to avoid bias
df1 = df1.drop_duplicates(subset=['Payload'])

# Removing special characters except SQLi or XSS detection relevant ones like <, >, ', --)
import re
df1['Payload'] = df1['Payload'].apply(lambda x: re.sub(r'[^a-zA-Z0-9<>"\'=%-]', ' ', x))

#Tokenization of the payload into words or symbols to isolate meaningful patterns (e.g., keywords and operators
# like SQLi keywords such as SELECT, INSERT..; Operators like =, !=, >, <, while XSS keywords such as script, alert, onerror <script>,
# and operators such as =, ;, (), <>.

df1['Payload_Tokens'] = df1['Payload'].apply(word_tokenize)

# Removal of common stopwords that do not really add significant meaning like "and", "or", "is"

stop_words = set(stopwords.words('english'))
df1['Payload_Cleaned'] = df1['Payload_Tokens'].apply(
    lambda tokens: [word for word in tokens if word not in stop_words]
)

#Making a copy of the dataFrame before TF_IDF vectorization of Payload column
df2 = df1.copy()

# Combination of pre-built stop word lists with domain-specific custom stop words for better control so
#balances general and domain-specific filtering for the Payload data. Since the payloads contains structured or non-standard
#text (e.g., www, http), additional preprocessing or custom stop words helps  and takes
#care of technical termms like xss, sql that may not be part of standard stop word lists.

# Combine NLTK and custom stop words 9will be used in vectorization also)
custom_stop_words = {'www', 'http', 'https', 'xssed', 'xss', 'sql'}
combined_stop_words = list(set(stopwords.words('english')).union(custom_stop_words))

# Balancing the dataset by oversampling or undersampling to enable fair representation of SQLi, XSS, and normal cases.
from sklearn.utils import resample
sql_df2 = df2[df2['SQLInjection'] == 1]
xss_df2 = df2[df2['XSS'] == 1]
normal_df2 = df2[df2['Normal'] == 1]

# Upsample minority class
sql_df2_upsampled = resample(sql_df2, replace=True, n_samples=len(normal_df2), random_state=42)
xss_df2_upsampled = resample(xss_df2, replace=True, n_samples=len(normal_df2), random_state=42)
normal_df2_upsampled = resample(normal_df2, replace=True, n_samples=len(normal_df2), random_state=42)

df2 = pd.concat([sql_df2_upsampled, xss_df2_upsampled, normal_df2_upsampled])

# Creating new dataFrame after resetting the index

df3 = df2.reset_index(drop=True)
df3.head(5)