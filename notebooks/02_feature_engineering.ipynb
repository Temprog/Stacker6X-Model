{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "r8SUdaNjXfuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports & Setup\n",
        "This notebook imports all necessary libraries and modules using `from utils.imports import *`, which centralizes all dependencies required for training. See `utils/imports.py` for full details.\n"
      ],
      "metadata": {
        "id": "CbcAtKgAuYwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports from utils.imports\n",
        "from utils.imports import pd, TfidfVectorizer, nltk, stopwords"
      ],
      "metadata": {
        "id": "PRPusRBnMrOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary. This variable will store the accuracy scores of the different models as a library. {} represents an empty library.\n",
        "# eg, when logistic regression is used, \"model_accuracy['Logistic Regression'] = 92.45\"  adds it as an entry to the dictionary\n",
        "\n",
        "model_accuracy = {}"
      ],
      "metadata": {
        "id": "NKQmc2MFyZ1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature creation\n",
        "\n",
        "## Add a new column 'Payload_Length' that contains the length of each payload\n",
        "df3['Payload_Length'] = df3['Payload_Cleaned'].apply(len)\n",
        "\n",
        "## Display the first few rows to confirm the new column is added\n",
        "print(df3.head(5))\n"
      ],
      "metadata": {
        "id": "PQao0ZKY7d4W",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607adef0-0f29-499b-ac78-491fc600a8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Payload  SQLInjection  XSS  \\\n",
            "0  ghjv9ef1y69cd6i59ihp6u3rsihkkx4z40nkyoqsdam1iq...             1    0   \n",
            "1  -5420'   union all select 2508 2508 2508 2508 ...             1    0   \n",
            "2  -2857%'       union all select 7167 7167 7167 ...             1    0   \n",
            "3  ssssssssssssssssssssssssssssssssssssssssssssss...             1    0   \n",
            "4  j95utpnafk32s451w4kxzhahkqzs98irp97aesd5n68axy...             1    0   \n",
            "\n",
            "   Normal                                     Payload_Tokens  \\\n",
            "0       0  [ghjv9ef1y69cd6i59ihp6u3rsihkkx4z40nkyoqsdam1i...   \n",
            "1       0  [-5420, ', union, all, select, 2508, 2508, 250...   \n",
            "2       0  [-2857, %, ', union, all, select, 7167, 7167, ...   \n",
            "3       0  [sssssssssssssssssssssssssssssssssssssssssssss...   \n",
            "4       0  [j95utpnafk32s451w4kxzhahkqzs98irp97aesd5n68ax...   \n",
            "\n",
            "                                     Payload_Cleaned  Payload_Length  \n",
            "0  [ghjv9ef1y69cd6i59ihp6u3rsihkkx4z40nkyoqsdam1i...              18  \n",
            "1  [-5420, ', union, select, 2508, 2508, 2508, 25...              11  \n",
            "2  [-2857, %, ', union, select, 7167, 7167, 7167,...              20  \n",
            "3  [sssssssssssssssssssssssssssssssssssssssssssss...              22  \n",
            "4  [j95utpnafk32s451w4kxzhahkqzs98irp97aesd5n68ax...              33  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection\n",
        "\n",
        "# Removing too short payloads less than 3 characters that may lack meaningful impact.\n",
        "df3 = df3[df3['Payload_Length'] > 3]"
      ],
      "metadata": {
        "id": "e-MN2gkyARrz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the longest and shortest payload lengths\n",
        "longest_length = df3['Payload_Length'].max()\n",
        "shortest_length = df3['Payload_Length'].min()\n",
        "\n",
        "# Display the results\n",
        "print(f\"The longest payload length is: {longest_length}\")\n",
        "print(f\"The shortest payload length is: {shortest_length}\")\n"
      ],
      "metadata": {
        "id": "ZmrmcPr47-vj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbcbea7d-b2e6-460b-ff0e-7778e4b042f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The longest payload length is: 269\n",
            "The shortest payload length is: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction\n",
        "\n",
        "# Vectorization (TF-IDF: Helps prioritize unique terms)\n",
        "# Initialize TfidfVectorizer\n",
        "\n",
        "# This step is needed to prevent the model from been hard coded to just 10500, else when the custom model is used, it will always\n",
        "# expect 10500 features as input for X (ie the fitted vectorized input/independent value).\n",
        "\n",
        "# Dynamically calculates max_features based on sample size\n",
        "sample_size = df1.shape[0]  # Get the number of rows in df1\n",
        "max_features = min(10500, sample_size)  # Limit max_features to sample size or 10500, whichever is smaller\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "   stop_words='english',  # This can also be replaced with custom stop_words when necessary\n",
        "   max_features=max_features,  # Use the calculated max_features\n",
        "   ngram_range=(1, 2),   # Unigrams and bigrams\n",
        "   min_df=2,             # Ignore terms with low frequency\n",
        "   max_df=0.9            # Ignore terms with very high frequency\n",
        ")\n",
        "\n",
        "# Note: The filtering parameters (min_df and max_df) are restrictive for small (simulated) dataset as tfidf_vectorizer is initialized with min_df=2 and max_df=0.9, meaning for\n",
        "# min_df=2: A term must appear in at least 2 documents (or eg web requests like url, query, method input or in Payload) to be included in the vocabulary (vectorization).\n",
        "# max_df=0.9: A term that appears in more than 90% of the documents is excluded. If this is not met, the model\n",
        "# will flag an error otherwise, min_df can be changed to 1. In the case of a large dataset, this will result to a very high-dimensional feature space, potentially containing many irrelevant or very rare terms, noise, overfitting, etc"
      ],
      "metadata": {
        "id": "bPsXIx5u6DvD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}